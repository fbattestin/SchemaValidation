{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idyNg1CAWbfq",
        "outputId": "a8578a40-d843-4257-a4e9-fde47e6a5c45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [Connected to cloud.r-\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [2 InRelease 3,626 B/3\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [Connecting to ppa.lau\r                                                                                                    \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r0% [3 InRelease 12.7 kB/119 kB 11%] [Connecting to security.ubuntu.com (185.125.190.36)] [Connecting\r                                                                                                    \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [645 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,631 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,617 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,308 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,595 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,047 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,340 kB]\n",
            "Fetched 9,418 kB in 2s (4,503 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pip install fastjsonschema"
      ],
      "metadata": {
        "id": "xKFhFTN4cJu-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "# Inicializa a sessão Spark\n",
        "spark = SparkSession.builder.appName(\"json_to_df\").getOrCreate()\n",
        "\n",
        "# Definir o esquema conforme discutido anteriormente\n",
        "meta_schema = StructType([\n",
        "    StructField(\"field1\", IntegerType(), True),\n",
        "    StructField(\"field2\", StringType(), True)\n",
        "])\n",
        "\n",
        "data_element_schema = StructType([\n",
        "    StructField(\"data1\", StringType(), True),\n",
        "    StructField(\"data2\", IntegerType(), False)\n",
        "])\n",
        "\n",
        "data_schema = ArrayType(data_element_schema)\n",
        "\n",
        "complete_schema = StructType([\n",
        "    StructField(\"meta\", meta_schema, True),\n",
        "    StructField(\"data\", data_schema, True)\n",
        "])\n",
        "\n",
        "# JSON fornecido\n",
        "json_data = '{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\"}]}'\n",
        "\n",
        "# Criar um RDD com o JSON\n",
        "rdd = spark.sparkContext.parallelize([json_data])\n",
        "\n",
        "# Converter o RDD para DataFrame usando o esquema definido\n",
        "df = spark.read.json(rdd, schema=complete_schema)\n",
        "\n",
        "# Exibir o DataFrame\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7chSqVTRXqD0",
        "outputId": "439f9805-992f-49bb-c4a9-f9879d7433d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|  meta|                data|\n",
            "+------+--------------------+\n",
            "|{1, a}|[{a, 1}, {a, 1}, ...|\n",
            "+------+--------------------+\n",
            "\n",
            "root\n",
            " |-- meta: struct (nullable = true)\n",
            " |    |-- field1: integer (nullable = true)\n",
            " |    |-- field2: string (nullable = true)\n",
            " |-- data: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- data1: string (nullable = true)\n",
            " |    |    |-- data2: integer (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fastjsonschema\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "# Inicializa a sessão Spark\n",
        "spark = SparkSession.builder.appName(\"json_to_df\").getOrCreate()\n",
        "json_data = {\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\", \"data2\": 1}]}\n",
        "json_str_bad = '{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\"}]}'\n",
        "json_str_god = '{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\", \"data2\": 1}]}'\n",
        "json_str_ybad = '{\"meta\": {\"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\"}]}'\n",
        "\n",
        "data = [(1,\"a\", json_str_god),\n",
        "        (1,\"a\", json_str_bad),\n",
        "        (1,\"a\", json_str_god),\n",
        "        (1,\"a\", json_str_god),\n",
        "        (1,\"a\", json_str_ybad),\n",
        "        (1,\"a\", json_str_god)]\n",
        "\n",
        "# Definir o esquema conforme discutido anteriormente\n",
        "meta_schema = StructType([\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"topic\", StringType(), True),\n",
        "    StructField(\"value\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, meta_schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "# from pyspark.sql.functions import get_json_object\n",
        "\n",
        "# df = df.select(\"key\",\n",
        "#                get_json_object(df.value, \"$.data\").alias(\"data\"))\n",
        "# df.show()\n",
        "# df.printSchema()\n",
        "\n",
        "\n",
        "point_schema = {\n",
        "  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"meta\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"field1\": {\n",
        "          \"type\": \"integer\"\n",
        "        },\n",
        "        \"field2\": {\n",
        "          \"type\": \"string\"\n",
        "        }\n",
        "      },\n",
        "      \"required\": [\"field1\", \"field2\"]\n",
        "    },\n",
        "    \"data\": {\n",
        "      \"type\": \"array\",\n",
        "      \"items\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"data1\": {\n",
        "            \"type\": \"string\"\n",
        "          },\n",
        "          \"data2\": {\n",
        "            \"type\": \"integer\"\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"data1\", \"data2\"]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"required\": [\"meta\", \"data\"]\n",
        "}\n",
        "point_validator = fastjsonschema.compile(point_schema)\n",
        "\n",
        "try:\n",
        "    point_validator(json_data)\n",
        "except fastjsonschema.JsonSchemaException as e:\n",
        "    print(dir(e))\n",
        "    print(f\"Data failed validation: {e}\")\n",
        "    print(e.name)\n",
        "    print(e.path, type(e.path))\n",
        "    print(e.rule)\n",
        "    print(e.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-BHfGQ-ZPFh",
        "outputId": "c6278c77-dd9e-4f89-e519-053664bbb3c9"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+\n",
            "|key|topic|               value|\n",
            "+---+-----+--------------------+\n",
            "|  1|    a|{\"meta\": {\"field1...|\n",
            "|  1|    a|{\"meta\": {\"field1...|\n",
            "|  1|    a|{\"meta\": {\"field1...|\n",
            "|  1|    a|{\"meta\": {\"field1...|\n",
            "|  1|    a|{\"meta\": {\"field2...|\n",
            "|  1|    a|{\"meta\": {\"field1...|\n",
            "+---+-----+--------------------+\n",
            "\n",
            "root\n",
            " |-- key: integer (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- value: string (nullable = true)\n",
            "\n",
            "['__cause__', '__class__', '__context__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__suppress_context__', '__traceback__', '__weakref__', 'args', 'definition', 'message', 'name', 'path', 'rule', 'rule_definition', 'value', 'with_traceback']\n",
            "Data failed validation: data.data[0] must contain ['data1'] properties\n",
            "data.data[0]\n",
            "['data', 'data', '0'] <class 'list'>\n",
            "required\n",
            "{'data2': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, explode\n",
        "import json\n",
        "\n",
        "class SchemaConverter:\n",
        "    \"\"\"\n",
        "    Class to convert standard JSON-schema (json-schema.org) to pyspark.sql.types.StructType.\n",
        "\n",
        "    Supported types are: \"string\", \"number\" (double), \"float\", \"integer\" (long), \"boolean\", \"object\" and \"array\".\n",
        "    The root of the JSON-schema is required to be:\n",
        "     - type 'object' and to have a field 'properties' having the contents of the schema\n",
        "     - or a field 'definitions' and '$ref'-field with reference to a main structure inside 'definitions'\n",
        "    In order to use such a reference in the schema the corresponding field name should be '$ref'.\n",
        "    The value is the address in the definitions, i.e. the path following the first occurrence of '#/definitions/'\n",
        "    will be applied on the definitions field. So a reference will look like: \"$ref\": \"#/definitions/path/to/struct\"\n",
        "\n",
        "    There isn't validation of input JSON-schema.\n",
        "    \"\"\"\n",
        "\n",
        "    # Supported simple types\n",
        "    SimpleTypeMap = {\n",
        "        \"string\": StringType(),\n",
        "        \"number\": DoubleType(),\n",
        "        \"float\": FloatType(),\n",
        "        \"integer\": IntegerType(),\n",
        "        \"boolean\": BooleanType()\n",
        "    }\n",
        "\n",
        "    def __init__(self, js: dict):\n",
        "        # Original standard JSON-schema (json-schema.org)\n",
        "        self.json_schema = js\n",
        "        # Main element to build schema\n",
        "        if \"$ref\" in self.json_schema:\n",
        "            self._main_element = self._get_ref_object(self.json_schema.get(\"$ref\"))\n",
        "        elif \"properties\" in self.json_schema:\n",
        "            self._main_element = self.json_schema\n",
        "        else:\n",
        "            raise Exception(\"Format of JSON-Schema is not recognized by JsonToSparkSchemaConverter\")\n",
        "        # Converting schema\n",
        "        self._build_schema_from_json()\n",
        "\n",
        "    def _get_ref_object(self, ref: str) -> dict:\n",
        "        \"\"\"\n",
        "        Get object via $ref\n",
        "        :param ref: internal reference to element, f.e. '#/definitions/path_to_element'\n",
        "        :return: element via reference\n",
        "        \"\"\"\n",
        "        path = ref.split('/')\n",
        "        result = self.json_schema\n",
        "        for i in range(1, len(path)):\n",
        "            result = result.get(path[i])\n",
        "        return result\n",
        "\n",
        "    def _get_simple_field(self, _type: str) -> StructType():\n",
        "        \"\"\"\n",
        "        Returns StructType() for simple type\n",
        "        \"\"\"\n",
        "        return self.SimpleTypeMap.get(_type)\n",
        "\n",
        "    def _get_oneof_field(self, oneof_list: list) -> StructType():\n",
        "        \"\"\"\n",
        "        Returns StructType() for 'oneOf' constructions.\n",
        "        F.e. {\"oneOf\":[{\"type\":\"null\"},{\"type\":\"integer\"}]}\n",
        "        \"\"\"\n",
        "        for item in oneof_list:\n",
        "            if \"type\" in item.keys():\n",
        "                if item.get(\"type\") in self.SimpleTypeMap:\n",
        "                    return self._get_simple_field(item.get(\"type\"))\n",
        "                elif item.get(\"type\") == \"array\":\n",
        "                    return self._get_array(item.get(\"items\"))\n",
        "                elif item.get(\"type\") == \"object\":\n",
        "                    return self._get_object(item.get(\"properties\"))\n",
        "            elif \"$ref\" in item.keys():\n",
        "                obj = self._get_ref_object(item.get(\"$ref\"))\n",
        "                return self._get_object(obj.get(\"properties\"))\n",
        "        raise Exception('oneOf field is incorrect')\n",
        "\n",
        "    def _get_array(self, _items: dict) -> StructType():\n",
        "        \"\"\"\n",
        "        Returns StructType() for 'array'.\n",
        "        F.e. {\"type\":\"array\",\"items\":{\"type\":\"string\"}}\n",
        "        \"\"\"\n",
        "        if \"type\" in _items:\n",
        "            if _items.get(\"type\") in self.SimpleTypeMap:\n",
        "                return ArrayType(self.SimpleTypeMap.get(_items.get(\"type\")))\n",
        "            elif _items.get(\"type\") == \"array\":\n",
        "                return ArrayType(self._get_array(_items.get(\"items\")))\n",
        "            elif _items.get(\"type\") == \"object\":\n",
        "                return ArrayType(self._get_object(_items.get(\"properties\")))\n",
        "            else:\n",
        "                raise Exception(\"Array element's type is not supported by JsonToSparkSchemaConverter\")\n",
        "        elif \"$ref\" in _items:\n",
        "            obj = self._get_ref_object(_items.get(\"$ref\"))\n",
        "            array_type = self._get_object(obj.get(\"properties\"))\n",
        "            return ArrayType(array_type)\n",
        "        else:\n",
        "            raise Exception('Array type is not supported by JsonToSparkSchemaConverter')\n",
        "\n",
        "    def _get_object(self, _properties: dict) -> StructType():\n",
        "        \"\"\"\n",
        "        Returns StructType() for 'object'\n",
        "        :param _properties: content of the 'properties'-field\n",
        "        \"\"\"\n",
        "        result = StructType()\n",
        "        for filed_name, field_info in _properties.items():\n",
        "            if \"type\" in field_info:\n",
        "                _type = field_info.get(\"type\")\n",
        "                if _type in self.SimpleTypeMap:\n",
        "                    result = result.add(filed_name, self._get_simple_field(_type))\n",
        "                elif _type == \"array\":\n",
        "                    result = result.add(filed_name, self._get_array(field_info.get(\"items\")))\n",
        "                elif _type == \"object\":\n",
        "                    result = result.add(filed_name, self._get_object(field_info.get(\"properties\")))\n",
        "            elif \"oneOf\" in field_info:\n",
        "                result = result.add(filed_name, self._get_oneof_field(field_info.get(\"oneOf\")))\n",
        "            else:\n",
        "                raise Exception('Type is not supported by JsonToSparkSchemaConverter')\n",
        "        return result\n",
        "\n",
        "    def _build_schema_from_json(self):\n",
        "        \"\"\"\n",
        "        Main builder for schema\n",
        "        \"\"\"\n",
        "        self.schema_for_spark = self._get_object(self._main_element.get(\"properties\"))\n",
        "\n",
        "converter = SchemaConverter(point_schema)\n",
        "# print(converter.schema_for_spark)\n",
        "schema = converter.schema_for_spark\n",
        "\n",
        "\n",
        "@udf(schema)\n",
        "def json_to_dict_list(json_str):\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        return []\n",
        "\n",
        "@udf(StringType())\n",
        "def validate_schema(json_str):\n",
        "  try:\n",
        "      point_validator = fastjsonschema.compile(point_schema)\n",
        "      point_validator(json.loads(json_str))\n",
        "      return None\n",
        "  except (json.JSONDecodeError, fastjsonschema.JsonSchemaException) as e:\n",
        "      return f\"Data failed validation: {e}\"\n",
        "\n",
        "\n",
        "point_validator = fastjsonschema.compile(point_schema)\n",
        "\n",
        "# # Aplicar a UDF para transformar a coluna JSON em uma coluna de lista\n",
        "df_ = df.withColumn(\"value_list\", json_to_dict_list(df[\"value\"]))\n",
        "df_.show()\n",
        "df_.printSchema()\n",
        "\n",
        "df__ = df_.withColumn(\"schema_validate\", validate_schema(df_[\"value\"]))\n",
        "df__.show(truncate=False)\n",
        "df__.printSchema()\n",
        "a = df__.select(\"value_list.meta\",\"schema_validate\", \"value_list.data\")\n",
        "a.printSchema()\n",
        "\n",
        "exploded_df = a.select(\"meta.field1\", \"schema_validate\", explode(a.data).alias(\"data_item\"))\n",
        "exploded_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql2GOalQjtfI",
        "outputId": "f75242af-ea1e-45b2-a200-204e73739702"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+--------------------+\n",
            "|key|topic|               value|          value_list|\n",
            "+---+-----+--------------------+--------------------+\n",
            "|  1|    a|{\"meta\": {\"field1...|{{1, a}, [{a, 1},...|\n",
            "|  1|    a|{\"meta\": {\"field1...|{{1, a}, [{a, 1},...|\n",
            "|  1|    a|{\"meta\": {\"field1...|{{1, a}, [{a, 1},...|\n",
            "|  1|    a|{\"meta\": {\"field1...|{{1, a}, [{a, 1},...|\n",
            "|  1|    a|{\"meta\": {\"field1...|{{1, a}, [{a, 1},...|\n",
            "+---+-----+--------------------+--------------------+\n",
            "\n",
            "root\n",
            " |-- key: integer (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- value: string (nullable = true)\n",
            " |-- value_list: struct (nullable = true)\n",
            " |    |-- meta: struct (nullable = true)\n",
            " |    |    |-- field1: integer (nullable = true)\n",
            " |    |    |-- field2: string (nullable = true)\n",
            " |    |-- data: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- data1: string (nullable = true)\n",
            " |    |    |    |-- data2: integer (nullable = true)\n",
            "\n",
            "+---+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------+\n",
            "|key|topic|value                                                                                                                                                          |value_list                                   |schema_validate                                                       |\n",
            "+---+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------+\n",
            "|1  |a    |{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\", \"data2\": 1}]}|{{1, a}, [{a, 1}, {a, 1}, {a, 1}, {a, 1}]}   |null                                                                  |\n",
            "|1  |a    |{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\"}]}            |{{1, a}, [{a, 1}, {a, 1}, {a, 1}, {a, null}]}|Data failed validation: data.data[3] must contain ['data2'] properties|\n",
            "|1  |a    |{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\", \"data2\": 1}]}|{{1, a}, [{a, 1}, {a, 1}, {a, 1}, {a, 1}]}   |null                                                                  |\n",
            "|1  |a    |{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\", \"data2\": 1}]}|{{1, a}, [{a, 1}, {a, 1}, {a, 1}, {a, 1}]}   |null                                                                  |\n",
            "|1  |a    |{\"meta\": {\"field1\": 1, \"field2\": \"a\"}, \"data\": [{\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1}, {\"data1\": \"a\", \"data2\": 1},{\"data1\": \"a\", \"data2\": 1}]}|{{1, a}, [{a, 1}, {a, 1}, {a, 1}, {a, 1}]}   |null                                                                  |\n",
            "+---+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- key: integer (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- value: string (nullable = true)\n",
            " |-- value_list: struct (nullable = true)\n",
            " |    |-- meta: struct (nullable = true)\n",
            " |    |    |-- field1: integer (nullable = true)\n",
            " |    |    |-- field2: string (nullable = true)\n",
            " |    |-- data: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- data1: string (nullable = true)\n",
            " |    |    |    |-- data2: integer (nullable = true)\n",
            " |-- schema_validate: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- meta: struct (nullable = true)\n",
            " |    |-- field1: integer (nullable = true)\n",
            " |    |-- field2: string (nullable = true)\n",
            " |-- schema_validate: string (nullable = true)\n",
            " |-- data: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- data1: string (nullable = true)\n",
            " |    |    |-- data2: integer (nullable = true)\n",
            "\n",
            "+-----------+--------------------+---------+\n",
            "|meta.field1|     schema_validate|data_item|\n",
            "+-----------+--------------------+---------+\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|Data failed valid...|   {a, 1}|\n",
            "|          1|Data failed valid...|   {a, 1}|\n",
            "|          1|Data failed valid...|   {a, 1}|\n",
            "|          1|Data failed valid...|{a, null}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "|          1|                null|   {a, 1}|\n",
            "+-----------+--------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode\n",
        "\n",
        "# Assumindo que 'point_schema' e 'df' já estão definidos\n",
        "point_validator = fastjsonschema.compile(point_schema)\n",
        "\n",
        "@udf(schema)\n",
        "def json_to_dict_list(json_str):\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        return []\n",
        "\n",
        "@udf(StringType())\n",
        "def validate_schema(json_str):\n",
        "    try:\n",
        "        point_validator(json.loads(json_str))\n",
        "        return None\n",
        "    except (json.JSONDecodeError, fastjsonschema.JsonSchemaException) as e:\n",
        "        return '.'.join(e.path)\n",
        "\n",
        "\n",
        "# Aplicar as transformações e explodir o DataFrame em uma única cadeia de operações\n",
        "exploded_df = (df\n",
        "               .withColumn(\"value_list\", json_to_dict_list(col(\"value\")))\n",
        "               .withColumn(\"schema_validate\", validate_schema(col(\"value\")))\n",
        "               .select(\"value_list.meta.field1\", \"schema_validate\", explode(\"value_list.data\").alias(\"data_item\")))\n",
        "\n",
        "# Exibir o resultado final (opcional para depuração)\n",
        "exploded_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH17uLgZul0Q",
        "outputId": "4e5ab456-317d-4791-c8e8-8bbb15ec0e54"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+---------------+---------+\n",
            "|value_list.meta.field1|schema_validate|data_item|\n",
            "+----------------------+---------------+---------+\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|    data.data.3|   {a, 1}|\n",
            "|                     1|    data.data.3|   {a, 1}|\n",
            "|                     1|    data.data.3|   {a, 1}|\n",
            "|                     1|    data.data.3|{a, null}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                     1|           null|   {a, 1}|\n",
            "|                  null|      data.meta|   {a, 1}|\n",
            "|                  null|      data.meta|   {a, 1}|\n",
            "|                  null|      data.meta|   {a, 1}|\n",
            "|                  null|      data.meta|{a, null}|\n",
            "+----------------------+---------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, from_json, monotonically_increasing_id\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Utilizar from_json para converter a string JSON em uma coluna struct\n",
        "json_df = df.withColumn(\"index\", monotonically_increasing_id()).withColumn(\"json_struct\", from_json(col(\"value\"), schema))\n",
        "\n",
        "# Extrair campos necessários da struct JSON\n",
        "extracted_df = json_df.select(\n",
        "    col(\"index\"),\n",
        "    col(\"json_struct.*\"),\n",
        "    col(\"value\"),\n",
        "    validate_schema(col(\"value\")).alias('is_valid')\n",
        ")\n",
        "extracted_df.show()\n",
        "extracted_df.printSchema()\n",
        "# extracted_df.select(col(extracted_df.is_valid))\n",
        "# Explodir o DataFrame com base no array 'data'\n",
        "exploded_df = extracted_df.select(\n",
        "    col(\"index\"),\n",
        "    col(\"meta.field1\"),\n",
        "    explode(col(\"data\")).alias(\"data_item\"),\n",
        "    col(\"value\")\n",
        ")\n",
        "\n",
        "# Validar schema apenas nos dados necessários\n",
        "validated_df = exploded_df.withColumn(\n",
        "    \"schema_validate\",\n",
        "    validate_schema(col(\"value\"))\n",
        ").drop(\"value\")\n",
        "\n",
        "# Exibir o resultado (com limitação para depuração)\n",
        "validated_df.select(\"*\", \"data_item.data2\").show(truncate=False, n=20)\n",
        "\n",
        "validated_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tqWZxJ7wC0E",
        "outputId": "e4dd9d04-c457-4651-c422-b387cf4ed826"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+--------------------+--------------------+-----------+\n",
            "|     index|     meta|                data|               value|   is_valid|\n",
            "+----------+---------+--------------------+--------------------+-----------+\n",
            "|         0|   {1, a}|[{a, 1}, {a, 1}, ...|{\"meta\": {\"field1...|       null|\n",
            "|         1|   {1, a}|[{a, 1}, {a, 1}, ...|{\"meta\": {\"field1...|data.data.3|\n",
            "|         2|   {1, a}|[{a, 1}, {a, 1}, ...|{\"meta\": {\"field1...|       null|\n",
            "|8589934592|   {1, a}|[{a, 1}, {a, 1}, ...|{\"meta\": {\"field1...|       null|\n",
            "|8589934593|{null, a}|[{a, 1}, {a, 1}, ...|{\"meta\": {\"field2...|  data.meta|\n",
            "|8589934594|   {1, a}|[{a, 1}, {a, 1}, ...|{\"meta\": {\"field1...|       null|\n",
            "+----------+---------+--------------------+--------------------+-----------+\n",
            "\n",
            "root\n",
            " |-- index: long (nullable = false)\n",
            " |-- meta: struct (nullable = true)\n",
            " |    |-- field1: integer (nullable = true)\n",
            " |    |-- field2: string (nullable = true)\n",
            " |-- data: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- data1: string (nullable = true)\n",
            " |    |    |-- data2: integer (nullable = true)\n",
            " |-- value: string (nullable = true)\n",
            " |-- is_valid: string (nullable = true)\n",
            "\n",
            "+----------+-----------+---------+---------------+-----+\n",
            "|index     |meta.field1|data_item|schema_validate|data2|\n",
            "+----------+-----------+---------+---------------+-----+\n",
            "|0         |1          |{a, 1}   |null           |1    |\n",
            "|0         |1          |{a, 1}   |null           |1    |\n",
            "|0         |1          |{a, 1}   |null           |1    |\n",
            "|0         |1          |{a, 1}   |null           |1    |\n",
            "|1         |1          |{a, 1}   |data.data.3    |1    |\n",
            "|1         |1          |{a, 1}   |data.data.3    |1    |\n",
            "|1         |1          |{a, 1}   |data.data.3    |1    |\n",
            "|1         |1          |{a, null}|data.data.3    |null |\n",
            "|2         |1          |{a, 1}   |null           |1    |\n",
            "|2         |1          |{a, 1}   |null           |1    |\n",
            "|2         |1          |{a, 1}   |null           |1    |\n",
            "|2         |1          |{a, 1}   |null           |1    |\n",
            "|8589934592|1          |{a, 1}   |null           |1    |\n",
            "|8589934592|1          |{a, 1}   |null           |1    |\n",
            "|8589934592|1          |{a, 1}   |null           |1    |\n",
            "|8589934592|1          |{a, 1}   |null           |1    |\n",
            "|8589934593|null       |{a, 1}   |data.meta      |1    |\n",
            "|8589934593|null       |{a, 1}   |data.meta      |1    |\n",
            "|8589934593|null       |{a, 1}   |data.meta      |1    |\n",
            "|8589934593|null       |{a, null}|data.meta      |null |\n",
            "+----------+-----------+---------+---------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- index: long (nullable = false)\n",
            " |-- meta.field1: integer (nullable = true)\n",
            " |-- data_item: struct (nullable = true)\n",
            " |    |-- data1: string (nullable = true)\n",
            " |    |-- data2: integer (nullable = true)\n",
            " |-- schema_validate: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}